{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cf6d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 48000/48000 [06:02<00:00, 132.35it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import preprocessing as pp\n",
    "\n",
    "df = pd.read_csv('/Users/donika/Desktop/images/metadata.csv')\n",
    "\n",
    "target_size = (128, 128)\n",
    "batch_size = 32\n",
    "\n",
    "train_folder_path = '/Users/donika/Desktop/images/datasets/train'\n",
    "val_folder_path = '/Users/donika/Desktop/images/datasets/validation'\n",
    "\n",
    "df_balanced = pp.balance_data(df)\n",
    "train_images, train_labels = pp.load_images_from_dataset(df_balanced, train_folder_path, target_size)\n",
    "print(\"Training samples after balancing:\", len(train_images))\n",
    "\n",
    "val_images, val_labels = pp.load_images_from_dataset(df, val_folder_path, target_size)\n",
    "print(\"Validation samples:\", len(val_images))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e63668",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder()\n",
    "encoder.fit(train_labels.reshape(-1, 1))\n",
    "\n",
    "train_labels_encoded = encoder.transform(train_labels.reshape(-1, 1)).toarray()\n",
    "val_labels_encoded = encoder.transform(val_labels.reshape(-1, 1)).toarray()\n",
    "\n",
    "print(\"Train Labels Shape:\", train_labels_encoded.shape)\n",
    "print(\"Validation Labels Shape:\", val_labels_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f8bd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow_model_optimization.sparsity import keras as sparsity\n",
    "import tensorflow_model_optimization as tfmot\n",
    "from kerastuner import HyperModel, Hyperband, Objective\n",
    "\n",
    "class PruningHyperModel(HyperModel):\n",
    "    def __init__(self, input_shape, num_classes):\n",
    "        self.input_shape = input_shape\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def build(self, hp):\n",
    "        model = models.Sequential([\n",
    "            layers.Conv2D(32, (3, 3), activation='relu', input_shape=self.input_shape),\n",
    "            layers.MaxPooling2D((2, 2)),\n",
    "            layers.BatchNormalization(),\n",
    "\n",
    "            layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "            layers.MaxPooling2D((2, 2)),\n",
    "            layers.BatchNormalization(),\n",
    "\n",
    "            layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "            layers.MaxPooling2D((2, 2)),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Flatten(),\n",
    "\n",
    "            layers.Dense(hp.Int('units1', min_value=64, max_value=512, step=64), activation='relu'),\n",
    "            layers.Dropout(hp.Float('dropout1', min_value=0.2, max_value=0.5, step=0.1)),\n",
    "            layers.Dense(hp.Int('units2', min_value=32, max_value=256, step=32), activation='relu'),\n",
    "            layers.Dropout(hp.Float('dropout2', min_value=0.2, max_value=0.5, step=0.1)),\n",
    "            layers.Dense(self.num_classes, activation='softmax')\n",
    "        ])\n",
    "\n",
    "        pruning_schedule = sparsity.PolynomialDecay(\n",
    "            initial_sparsity=hp.Float('initial_sparsity', min_value=0.2, max_value=0.8, step=0.1),\n",
    "            final_sparsity=hp.Float('final_sparsity', min_value=0.5, max_value=0.9, step=0.1),\n",
    "            begin_step=0,\n",
    "            end_step=1000\n",
    "        )\n",
    "\n",
    "        pruned_model = sparsity.prune_low_magnitude(model, pruning_schedule=pruning_schedule)\n",
    "\n",
    "        pruned_model.compile(optimizer='adam', loss='categorical_crossentropy',\n",
    "                             metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
    "\n",
    "        return pruned_model\n",
    "\n",
    "input_shape = (128, 128, 3)\n",
    "num_classes = 7\n",
    "\n",
    "# Create the tuner instance\n",
    "tuner = Hyperband(\n",
    "    hypermodel=PruningHyperModel(input_shape, num_classes),\n",
    "    objective=Objective('val_accuracy', direction='max'),\n",
    "    max_epochs=30,\n",
    "    factor=3,\n",
    "    directory='tuner_directory',\n",
    "    project_name='pruning_tuning'\n",
    ")\n",
    "\n",
    "tuner.search(train_images, train_labels_encoded, epochs=30, batch_size=32,\n",
    "             validation_data=(val_images, val_labels_encoded))\n",
    "\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "best_pruned_model = tuner.hypermodel.build(best_hps)\n",
    "best_pruned_model.compile(optimizer='adam', loss='categorical_crossentropy',\n",
    "                          metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3759ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "history=best_pruned_model.fit(train_images, train_labels_encoded, epochs=30, batch_size=32,\n",
    "                      validation_data=(val_images, val_labels_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbc4683",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
